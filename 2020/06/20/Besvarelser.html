<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Besvarelser | AI/Machine Learning</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Besvarelser" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Opgaver – Uge 6" />
<meta property="og:description" content="Opgaver – Uge 6" />
<link rel="canonical" href="https://rbje30990.github.io/Deep_Learning/2020/06/20/Besvarelser.html" />
<meta property="og:url" content="https://rbje30990.github.io/Deep_Learning/2020/06/20/Besvarelser.html" />
<meta property="og:site_name" content="AI/Machine Learning" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-20T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://rbje30990.github.io/Deep_Learning/2020/06/20/Besvarelser.html","@type":"BlogPosting","headline":"Besvarelser","dateModified":"2020-06-20T00:00:00-05:00","datePublished":"2020-06-20T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://rbje30990.github.io/Deep_Learning/2020/06/20/Besvarelser.html"},"description":"Opgaver – Uge 6","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/Deep_Learning/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://rbje30990.github.io/Deep_Learning/feed.xml" title="AI/Machine Learning" /><link rel="shortcut icon" type="image/x-icon" href="/Deep_Learning/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/Deep_Learning/">AI/Machine Learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/Deep_Learning/about/">About Me</a><a class="page-link" href="/Deep_Learning/search/">Search</a><a class="page-link" href="/Deep_Learning/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Besvarelser</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-06-20T00:00:00-05:00" itemprop="datePublished">
        Jun 20, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      23 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Opgaver – Uge 6</p>

<p>Opgave 2: Færdig</p>

<ol>
  <li>
    <p><strong>Name five areas where deep learning is now the best in the world.</strong></p>

    <ul>
      <li>
        <p>Computer vision,</p>
      </li>
      <li>
        <p>Image generation</p>
      </li>
      <li>
        <p>Playing games</p>
      </li>
      <li>
        <p>Recommendation system</p>
      </li>
      <li>
        <p>Medicine</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Based on the book of the same name, what are the requirements for parallel distributed processing (PDP)?</strong></p>

    <ul>
      <li>
        <p>Et set af proces enheder</p>
      </li>
      <li>
        <p>En tilstand af aktivering</p>
      </li>
      <li>
        <p>En outputfunktion for hver enhed</p>
      </li>
      <li>
        <p>Et mønster til tilslutning ” <em>pattern of connectivity</em>” mellem enheder</p>
      </li>
      <li>
        <p>En formerings regel til udbredelse af mønstre for aktiviteter gennem netværket af tilslutningsmuligheder</p>
      </li>
      <li>
        <p>En aktiveringsregel for at kombinerer de input som har en betydning for enheden med dens nuværende tilstand til at fremstille et output for enheden.</p>
      </li>
      <li>
        <p>En læringsregel hvor ”<em>pattern of connectivity</em>” bliver ændret af erfaring</p>
      </li>
      <li>
        <p>et miljø hvor systemet kan arbejde.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>What were the two theoretical misunderstandings that held back the field of neural networks?</strong></p>
  </li>
</ol>

<blockquote>
  <p>De antog at det ikke var muligt at bruge neural networks. De antog at kun et lager mere (altså 3) var nok til at lave et neural netværk</p>
</blockquote>

<ol>
  <li><strong>What is a GPU?</strong></li>
</ol>

<blockquote>
  <p>Det er special slags processor i en computer som kan håndtere tusindvis af opgaver påamme tid.</p>
</blockquote>

<ol>
  <li><strong>Why is it hard to use a traditional computer program to recognize images in a photo?</strong></li>
</ol>

<p>Traditionelle computere programmer er ikke bygget til at lærer, de er hardcodet til at lave en bestemt løsning frem for at lære. Hvor at aflæsning af billeder næsten er umulig at hardcode</p>

<ol>
  <li><strong>What did Samuel mean by “weight assignment”?</strong></li>
</ol>

<p>Er et særlig valg af værdier til de værdier som går ind og definerer hvordan programmet skal fungere</p>

<ol>
  <li><strong>What term do we normally use in deep learning for what Samuel called “weights”?</strong></li>
</ol>

<p>Modelparameter</p>

<ol>
  <li><strong>Draw a picture that summarizes Samuel’s view of a machine learning model.</strong></li>
</ol>

<blockquote>
  <p><img src="https://rbje30990.github.io/Deep_Learning/assets/img/2020-06-20-Besvarelser/media/image1.png" alt="" /></p>
</blockquote>

<ol>
  <li><strong>Why is it hard to understand why a deep learning model makes a particular prediction?</strong></li>
</ol>

<p>Den fungere meget som en blackboks, vi giver den noget data og får et resultat.</p>

<ol>
  <li><strong>What is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy?</strong></li>
</ol>

<p>Universal approximation theorem</p>

<ol>
  <li><strong>What do you need in order to train a model?</strong></li>
</ol>

<p>Data og labels</p>

<ol>
  <li><strong>How could a feedback loop impact the rollout of a predictive policing model?</strong></li>
</ol>

<p>Hvis modellens resultat vil medføre i yderligere inputs, kan denne input have en manipulations effekt på modellens kommende resultater som vil medføre til yderligere output af samme type og format. Dette giver en gentagende effekt og misviser virkeligheden.</p>

<ol>
  <li><strong>Do we always have to use 224×224-pixel images with the cat recognition model?</strong></li>
</ol>

<p>Man skulle i starten, men nu skal man ikke. Jo større billedet er jo bedre vil modellen virke</p>

<ol>
  <li>
    <p><strong>What is the difference between classification and regression?</strong></p>

    <ul>
      <li>
        <p>Classification modellen er en som prøver at forudsige en klasse eller kategori, hvor den forholder sig til en bestemt antal muligheder. Eksemplet er hund og kat modellen.</p>
      </li>
      <li>
        <p>Regression modellen er en som prøver at forudsige en eller flere numeriske mængder, så som temperatur eller en lokation.</p>
      </li>
    </ul>
  </li>
</ol>

<p>Opgave 3: Færdig</p>

<ol>
  <li><strong>What is a validation set? What is a test set? Why do we need them?</strong></li>
</ol>

<p><strong>Validation set, validere modellens data.</strong></p>

<p>Test set, yderligere validere modellens data unden noget bios.</p>

<ol>
  <li><strong>What will fastai do if you don’t provide a validation set?</strong></li>
</ol>

<p>Opdele selv data’en i tre dele</p>

<ol>
  <li><strong>Can we always use a random sample for a validation set? Why or why not?</strong></li>
</ol>

<p>Hvis noget er tid estimeret, kan du ikke hente data’en randomt</p>

<ol>
  <li><strong>What is overfitting? Provide an example.</strong></li>
</ol>

<blockquote>
  <p>Det er når modellen, i stedet for at finde mønster begynder at huske data’en i stedet for.</p>

  <p>Eksempel:</p>

  <p><img src="assets/img/2020-06-20-Besvarelser/media/image2.png" alt="Example of overfitting" /></p>
</blockquote>

<ol>
  <li><strong>What is a metric? How does it differ from “loss”?</strong></li>
</ol>

<p>Metric er en funktion der måler kvaliteten af modellens forudsigelser ved brug af validation sættet og vil blive printet ud i slutningen af hver Epochs.</p>

<p>Metric bliver aflæst af mennesker, så en god metric er en som er nem at forstå. Den minder meget om Loss, dog er den ikke det samme, siden loss definere ”measure of performance” som træningssystemet bruger til at opdatere weights automatisk.</p>

<ol>
  <li><strong>How can pretrained models help?</strong></li>
</ol>

<p>I stedet for at lave en model fra starten, så kan man benytte sig af en pretrained model, som allerede har lært nogle af de mønstre som man ønsker.</p>

<ol>
  <li><strong>What is the “head” of a model?</strong></li>
</ol>

<p>Det er hvad du tilføjer til den pretrained model således at den gør hvad du ønsker</p>

<ol>
  <li><strong>What kinds of features do the early layers of a CNN find? How about the later layers?</strong></li>
</ol>

<p>Det første lag finder generelle ting, så som former og figur.</p>

<p>Det sidste lag finder koncepter.</p>

<p>Jo flere lag jo mere specifik.</p>

<ol>
  <li><strong>Are image models only useful for photos?</strong></li>
</ol>

<p>Nej, den kan bruges til alt hvad der har et visuelt mønster.</p>

<ol>
  <li><strong>What is an “architecture”?</strong></li>
</ol>

<p>Den matematiske funktion som vi putter input parameter i.</p>

<ol>
  <li><strong>What is segmentation?</strong></li>
</ol>

<p>At skabe en model som kan genkende indholdet af hvert pixel i et billede bliver kaldt ”segmentation”</p>

<ol>
  <li><strong>What is y_range used for? When do we need it?</strong></li>
</ol>

<p>Det er brugerdefineret rækkevidde for hvilken rækkevidde den må gætte på.</p>

<ol>
  <li><strong>What are “hyperparameters”?</strong></li>
</ol>

<p>Parameter som er fastsat af udvikleren, som ikke kan ændres af modellen</p>

<ol>
  <li><strong>What’s the best way to avoid failures when using AI in an organization?</strong></li>
</ol>

<p>Det er vigtigt at du benytter dig af dine egne test data, når du modtager en pretrained model. For at sikre at den ikke er overfittet.</p>

<ol>
  <li>
    <p><strong>Complete the Jupyter Notebook online appendix. (use the file app_jupyter.ipynb in Gradient or get it from github: <a href="https://oreil.ly/9uPZe">https://oreil.ly/9uPZe</a>)</strong></p>
  </li>
  <li>
    <p><strong>Why is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning?</strong></p>
  </li>
</ol>

<p>En GPU kan løse mange flere løsninger på samme tid end hvad den CPU kan.</p>

<ol>
  <li><strong>Try to think of three areas where feedback loops might impact the use of machine learning. See if you can find documented examples of that happening in practice.</strong></li>
</ol>

<p>Politi eksemplet</p>

<p>Skak eksemplet</p>

<p>Uge 7</p>

<p>Opgave 1: Ikke færdig</p>

<p>Opsæt hver især en blog med fastpages på samme måde. Læg jeres besvarelser fra sidste gang ind på bloggen. Bloggen skal bruges som dokumentation til eksamen, og derfor vælger vi fastpages, for så kan vi lægge notebooks ind.</p>

<p>Opgave 2: Fejl ved udførelse</p>

<p>Lav samme løsning som bogen i kapitel 2, men i stedet for bjørne, skal I lave en Hotdog/Not Hotdog classifier, brug min kode og min api-nøgle til at hente billeder.</p>

<p>Problem: Opstår ved kald på results = search_images_bing(key, ’hotdog’) årsagen til dette er at den nøgle vi har fået udleveret, er ikke længere gyldig, fordi licensen er udløbet.</p>

<p>Uge 8</p>

<p>Opgave 3:</p>

<ol>
  <li><strong>Explain how the “pixel similarity” approach to classifying digits works.</strong></li>
</ol>

<p>Der laves et tensor stack, hvor der tages et average af hvert pixel, som så sammenlignes med det ideelle billed af et 3- eller 7- tal</p>

<ol>
  <li><strong>What is a list comprehension?</strong></li>
</ol>

<p>Syntax: list= [<em>expression</em> for item in iterable if condition]<img src="https://rbje30990.github.io/Deep_Learning/assets/img/2020-06-20-Besvarelser/media/image3.png" alt="" /></p>

<p>En list comprehension er en kort måde at lave en liste på.</p>

<p>Eksempler:</p>

<p><img src="https://rbje30990.github.io/Deep_Learning/assets/img/2020-06-20-Besvarelser/media/image4.png" alt="" /><br />
Create one now that selects odd numbers from a list and doubles them.</p>

<p>double_odd = [double_numbers(i) for i in numbers if i%2 == 1]</p>

<ol>
  <li><strong>What is a “rank-3 tensor”?</strong></li>
</ol>

<p>Det er en liste af rank-2 matrix</p>

<ol>
  <li><strong>What are RMSE and L1 norm?</strong></li>
</ol>

<blockquote>
  <p>Root Mean Square Error : forskellene mellem værdier forudsagt af en model eller en estimator og de observerede værdier.</p>
</blockquote>

<p>L1 norm : The L1 norm that is calculated as the sum of the absolute values of the vector.</p>

<ol>
  <li><strong>Create a 3×3 tensor or array containing the numbers from 1 to 9.<br />
Double it.</strong></li>
</ol>

<p><img src="https://rbje30990.github.io/Deep_Learning/assets/img/2020-06-20-Besvarelser/media/image5.png" alt="" /><br />
Select the bottom-right four numbers.</p>

<ol>
  <li><strong>What is broadcasting?</strong></li>
</ol>

<p>Det er sort magi der udvider tensor med end mindre rank til at matche den med en stør rank.</p>

<ol>
  <li><strong>Are metrics generally calculated using the training set, or the validation set?</strong></li>
</ol>

<p>Validation</p>

<ol>
  <li><strong>What is SGD?</strong></li>
</ol>

<p>Stochastic Gradient Descent :</p>

<blockquote>
  <p>Det er en optimizer der kan håndtere stor datasets idet den samler det i mini-batches</p>
</blockquote>

<ol>
  <li>
    <p><strong>What are the seven steps in SGD for machine learning?</strong></p>

    <ol>
      <li>
        <p>Init</p>
      </li>
      <li>
        <p>predict</p>
      </li>
      <li>
        <p>loss</p>
      </li>
      <li>
        <p>gradient</p>
      </li>
      <li>
        <p>step</p>
      </li>
      <li>
        <p>repeat?</p>
      </li>
      <li>
        <p>stop</p>
      </li>
    </ol>
  </li>
  <li>
    <p><strong>How do we initialize the weights in a model?</strong></p>
  </li>
</ol>

<p>vi tager random værdier for at starte med, da det alligevel bliver tilpasset</p>

<ol>
  <li><strong>What is “loss”?</strong></li>
</ol>

<p>loss is a number indicating how bad the model’s prediction was on a single epoch. If model prediction is perfect the loss is 0.</p>

<ol>
  <li><strong>Why can’t we always use a high learning rate?</strong></li>
</ol>

<p>Med en for stor learning rate, laver vi for store steps, hvormed vi ikke kan komme til bedste værdier. Den skyder over målet</p>

<ol>
  <li><strong>What is a “gradient”?</strong></li>
</ol>

<p>En gradient er værdien på et fald eller stigning fra et punkt på en graf.</p>

<p>“Gradient is denied as rise/run; that is, the change in the value of the function, divided by the change in the value of the parameter.”</p>

<ol>
  <li><strong>Why can’t we use accuracy as a loss function?</strong></li>
</ol>

<p>accuracy er for mennesker</p>

<p>loss bliver brugt for modellen</p>

<ol>
  <li><strong>What is the difference between a loss function and a metric?</strong></li>
</ol>

<p>loss er antallet af forkerte predictions, mens metric er optimeret for mennesker, det viser i % hvor højt scoren er i forhold til rigtige predictions</p>

<ol>
  <li><strong>What is the function to calculate new weights using a learning rate?</strong></li>
</ol>

<p>w -= gradient(w) * lr</p>

<ol>
  <li><strong>What does the backward method do?</strong></li>
</ol>

<p>backwards methoden laver backpropagation, som optimizer weights, og dermed “træner” modellen</p>

<p>Opgave 6: fejl ved udførelse</p>

<p>Arbejd videre med ”<em>Who painted the painting?</em>” (enten i Gradient eller Colab).<br />
Eksperimenter f.eks. med at tune modellen f.eks. vha. data-cleaning eller en anden arkitektur (anvend f.eks. r<em>esnet30</em> eller <em>resnet50</em> i stedet for <em>resnet18</em>).</p>

<p>Problem: samme problem som i uge 7 opgave 2.</p>

<p>Uge 9</p>

<p>Opgave 1:</p>

<p>Opgaven er: I skal bygge jeres egen learner fra bunden af ved at lave en klasse I Python der hedder MyLearner eller noget i den stil. Learneren skal indeholde de 7 punkter bogen beskriver på side 150, og som gennemgås af to omgange fra hhv. side 159-162 og igen side 171-176.</p>

<p>Lad være med at google løsningen, men kig i bogen og gør som de gør.</p>

<p>Opsæt derefter et neuralt netværk som vist på side 178, men eksperimenter med at gøre netværket dybere og/eller med flere noder pr lag, og se, hvad der performer bedst, ved at udskrive accuracy pr epoch.</p>

<p>Test med 3-tal og 7-tal fra MNIST (eller med dit eget hotdog-datasæt).</p>

<p>Opgave 2:</p>

<p>Fordel jer igen/stadig I break-out rooms, og lav opgave 2 i bogen kapitel 4 under Further Research. Brug din egen learner til opgaven, og træn den til at genkende alle tallene I MNIST (eller alle kunstnerne fra dit Maleri-datasæt).</p>

<p>Uge 10</p>

<p>Opgave 4:</p>

<p>Vælg en af følgende opgaver (eller kombiner efter eget ønske):</p>

<ul>
  <li>
    <p>See if you can improve the accuracy of the classifier in this chapter. What is the best accuracy you can achieve?</p>
  </li>
  <li>
    <p>Opgaven ”Who painted the painting?” er en multi-kategori opgave (en kategori for hver maler du har billeder fra).<br />
Brug den til at arbejde med de enkelte teknikker fra kapitlet (f.eks. DataBlock ,item resize, batch resize, RegexLabeller (måske skal du overveje navngivning af billedfilerne), summary, softmax, nll_loss, CrossEntropyLoss, lr_find, fine_tune, fit_one_cycle, unfreeze, valg af antal epochs mm.)</p>
  </li>
  <li>
    <p>Hvis dine data til ”Who painted the painting?” ikke er velegnede kan du finde et velegnet multi-kategori dataset online.</p>
  </li>
</ul>

<p>Uge 11</p>

<p>Opgave 1:</p>

<p>Opgaven i dag kommer til at løbe over to uger. I denne uge skal I skal bruge det I har lært indtil nu til at løse opgaven fra zindi.africa (se nedenfor). Upload jeres løsningsark til sitet og få en score tilbage. I næste uge, skal I så forsøge at forbedre resultatet ved at bruge de optimeringstricks, der bliver gennemgået i næste kapitel.</p>

<p>Opgaven findes her:</p>

<p><a href="https://zindi.africa/competitions/iclr-workshop-challenge-1-cgiar-computer-vision-for-crop-disease">https://zindi.africa/competitions/iclr-workshop-challenge-1-cgiar-computer-vision-for-crop-disease</a></p>

<p>Hvis datasættet driller, så hent det her:</p>

<p><a href="https://www.kaggle.com/shadabhussain/cgiar-computer-vision-for-crop-disease">https://www.kaggle.com/shadabhussain/cgiar-computer-vision-for-crop-disease</a></p>

<p>Her er den evaluation metric der bruges til at vurdere, hvor godt I har klaret opgaven:</p>

<p>LogLoss=−1n∑i=1n[yilog(ŷ i)+(1−yi)log(1−ŷ i)],</p>

<p>Findes den allerede i fastai? Hvis ikke kan I jo overveje, hvordan man implementerer den og får den vist per epoch. Og hvis den findes, kan I jo sætte den på.</p>

<p>Hvis I når at blive færdige med zindi.africa-opgaven, så kan I vælge mellem følgende to opgaver:</p>

<h2 id="besvar-en-recaptcha-med-kode">Besvar en recaptcha med kode</h2>

<p><img src="assets/img/2020-06-20-Besvarelser/media/image6.png" alt="Graphical user interface, application, Word Description automatically generated" /></p>

<p>I skal lave en løsning, hvor I skriver captions til billeder. Fx: Biler, skorsten, en brandhane, et fordgængerfelt etc.</p>

<p>Bagefter kan I jo prøve, om I via javascript kan downloade billederne fra en recaptcha og identificere dem korrekt med jeres model, og igen via javascript kan udvælge de rigtige billeder og trykke på submitknappen . Forskellen mellem et valgt billede og et ikke-valgt er blot en ekstra css-klasse. Se test-recaptchas her:</p>

<p><a href="https://www.google.com/recaptcha/api2/demo">https://www.google.com/recaptcha/api2/demo</a></p>

<p>Eller:</p>

<p><a href="https://patrickhlauke.github.io/recaptcha/">https://patrickhlauke.github.io/recaptcha/</a></p>

<h2 id="kaggle-competiton">Kaggle competiton</h2>

<p>Denne opgave er til jer, som gerne vil have lidt ekstra ved siden af SU’en. Opgaven bør kunne løses med det I har lært indtil nu, men første udfordring er faktisk at få udpakket zipfilen. Jeg vil anbefale at man udpakker dele af zipfilen fra flere forskellige tråde, men derfor kan det stadigvæk godt være det tager lang tid.</p>

<p>Konkurrencen er her:</p>

<p><a href="https://www.kaggle.com/c/bms-molecular-translation/overview">https://www.kaggle.com/c/bms-molecular-translation/overview</a></p>

<p>Uge 12</p>

<p>Opgave 1</p>

<ol>
  <li><strong>What is the difference between ImageNet and Imagenette? When is it better to experiment on one versus the other?</strong></li>
</ol>

<blockquote>
  <p>Imagenette er et datasæt lavet af 10 vidt forskellige klasser fra Imagenet. Imagenette er bedre at experimentere på når man vil træne hurtigere for at se hvad der fungere på det større også.</p>
</blockquote>

<ol>
  <li><strong>What is normalization?</strong></li>
</ol>

<blockquote>
  <p>At vores input data har et gennemsnit på 0 og en standardafvigelse på 1.</p>
</blockquote>

<ol>
  <li><strong>Why didn’t we have to care about normalization when using a pretrained model?</strong></li>
</ol>

<blockquote>
  <p>Her brugte vi CNN_learner som selv normaliserede vores data for os. Ellers kan det netop være vigtigt at vi normaliserer vores data på samme måde som er blevet gjort i forvejen.</p>
</blockquote>

<ol>
  <li><strong>What is progressive resizing?</strong></li>
</ol>

<blockquote>
  <p>Man starter med at træne sin model med små billeder, og ende med at træne med større billeder.</p>
</blockquote>

<ol>
  <li><strong>What is test time augmentation? How do you use it in fastai?</strong></li>
</ol>

<blockquote>
  <p>Ved skabelse af validerings sættet, kan man hakke hvert billede ud i flere dele og sende dem igennem modellen. Fastai vil som standard lave en center hakning af billedet, dette vil dog ofte medføre at kanterne i billedet ikke bliver taget med.</p>
</blockquote>

<ol>
  <li><strong>Is using TTA at inference slower or faster than regular inference? Why?</strong></li>
</ol>

<blockquote>
  <p>Det er langsommere fordi man skal inferere over flere augmentede billeder og finde gennemsnittet i stedet for at kun gøre det på et billede.</p>
</blockquote>

<ol>
  <li><strong>What is Mixup? How do you use it in fastai?</strong></li>
</ol>

<blockquote>
  <p>Det er et værktøj til at undgå overfitting.</p>

  <p>Man kan bruge det ved at sætte den på cbs(callbacks) i constructoren for vores learner.</p>
</blockquote>

<ol>
  <li><strong>Why does Mixup prevent the model from being too confident?</strong></li>
</ol>

<blockquote>
  <p>Den lægger to billeder over hinanden som modellen så skal predicte hvilke er og hvad deres vægt er.</p>
</blockquote>

<ol>
  <li><strong>Why does training with Mixup for five epochs end up worse than training without Mixup?</strong></li>
</ol>

<blockquote>
  <p>Fordi Mixup laver et nyt Mixup hver eneste gang man kigger på et billede, så efter 5 epoker har den ikke nået at kigge nok på de samme ting til at kunne se et mønster. Den skal angiveligt træne i over 80 epoker for at være bedre.</p>
</blockquote>

<ol>
  <li><strong>What is the idea behind label smoothing?</strong></li>
</ol>

<blockquote>
  <p>Det er at undgå overfitting, hvor label smoothing går ind og gør modellen mindre sikker på sine resultater. Derved bliver modellen bedre til at generalisere.</p>
</blockquote>

<ol>
  <li><strong>What problems in your data can label smoothing help with?</strong></li>
</ol>

<blockquote>
  <p>Det er mere robust i tilfælde af fejllabeled data.</p>
</blockquote>

<ol>
  <li><strong>When using label smoothing with five categories, what is the target associated with the index 1?</strong></li>
</ol>

<blockquote>
  <p>[0.01, 0.96, 0.01, 0.01, 0.01] = 0.96</p>
</blockquote>

<ol>
  <li><strong>What is the first step to take when you want to prototype quick experiments on a new dataset?</strong></li>
</ol>

<blockquote>
  <p>Start med at lave et lille subset af dit dataset, så du hurtigt kan experimenter med det, i stedet for at bruge et kæmpe dataset, der vil tage alt for lang tid at træne.</p>
</blockquote>

<p>Opgave 2</p>

<p>Fortsæt med <em>zindi.africa</em> opgaven fra sidste uge. Anvend emnerne fra denne uges pensum (normalization, progressive resizing, test time augmentation, mixup og label smoothing) og se om du får bedre resultater.</p>

<p>Uge 15:</p>

<p>Opgave 1</p>

<p>Med CL-strukturen tænk-par-del, skal I nu først, hver især, nedskrive jeres egen forståelse af følgende begreber. Til det får I 15 min.</p>

<p>Derefter sætter jeg jer sammen to og to, hvor I udveksler jeres forståelser, og bliver enige om en fælles forståelse af begreberne (I må gerne tage bogen eller andre ressourcer i brug til at hjælpe hukommelsen). Til dette er der 10 min.</p>

<p>Til sidst bliver I sat sammen parvis for endnu engang at tjekke jeres forståelser. Det får I 5 min til. På denne burde I få en vis mængde sikkerhed for, hvad begreberne dækker over. Der er selvfølgelig den mulighed at alle tager fejl, men der er stor sandsynlighed for, at I efter en snak vil være kommet frem til den korrekte forståelse fordi fire hoveder tænker bedre end ét J</p>

<p><strong>Begreberne er:</strong></p>

<p>Convolution</p>

<p>Kernel</p>

<p>Stride</p>

<p>Padding</p>

<p>Filter</p>

<p>Batchnorm</p>

<p>Opgave 3</p>

<p>Implementér op til tre af følgende arkitekturer, og sæt dem ind i en learner med et passende datasæt.</p>

<p>Første del af opgaven, er at forstå, hvordan man skal forstå billedet. Næste del at implementere det. Der bruges begreber, som I ikke har set før, så som Maxpool og Averagepool som vil blive delvist forklaret i næste kapitel af bogen (og i videoen ovenfor), men også ord som Full connection eller dense, som I måske har en idé om hvad betyder.</p>

<p>I må gerne læse den videnskabelige artikel bag arkitekturen, hvis det hjælper</p>

<p>1)  LeNet-5:  <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf">Gradient-based learning applied to document recognition</a></p>

<p><img src="assets/img/2020-06-20-Besvarelser/media/image7.png" alt="Screen-Shot-2018-04-16-at-11.34.51-AM" /></p>

<p>2)  AlexNet: <a href="https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a></p>

<p><img src="assets/img/2020-06-20-Besvarelser/media/image8.png" alt="AlexNet-CNN-architecture-layers" /></p>

<p>3)  VGG-16:  <a href="https://arxiv.org/abs/1409.1556">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></p>

<p><img src="assets/img/2020-06-20-Besvarelser/media/image9.png" alt="vgg16" /></p>

<p>Uge 16</p>

<p>Opgave 1</p>

<ol>
  <li><strong>What is a “skip connection”?</strong></li>
</ol>

<blockquote>
  <p>værdierne af et output bliver til inputtet på flere lag, hvor der ikke bliver gjort noget ved disse, lag bliver altså “skippet”, når en gradient kommer for tæt på 0.</p>
</blockquote>

<ol>
  <li><strong>Why do skip connections allow us to train deeper models?</strong></li>
</ol>

<blockquote>
  <p>Det gør det muligt at undgå gradient degradation, således at der ikke kan være for mange lag i en model.</p>
</blockquote>

<ol>
  <li><strong>What is “identity mapping”?</strong></li>
</ol>

<blockquote>
  <p>Returnerer inputtet uden at ændre det</p>
</blockquote>

<ol>
  <li><strong>What do ResNets have to do with residuals?</strong></li>
</ol>

<blockquote>
  <p>ResNets prøver ikke at komme med et output fra et layer, men i stedet for med at minimere differencen mellem outputtet og det ønskede resultat. Dermed er ResNets god til at lære om der er små ændringer eller om der skal skippes.</p>

  <p>“<em>If the outcome of a given layer is x and we’re using a ResNet block that returns y = x + block(x), we’re not asking the block to predict y; we are asking it to predict the difference between y and x. So the job of those blocks isn’t to predict certain fea‐ tures, but to minimize the error between x and the desired y. A ResNet is, therefore, good at learning about slight differences between doing nothing and passing through a block of two convolutional layers (with trainable weights). This is how these models got their name: they’re predicting residuals (reminder: “residual” is prediction minus target).</em>” - fra side 448</p>
</blockquote>

<ol>
  <li><strong>How do we deal with the skip connection when there is a stride-2 convolution? How about when the number of filters changes?</strong></li>
</ol>

<blockquote>
  <p>Ved at bruge <em>fully convolutional networks</em> hvor man tager gennemsnittet af activations henover et convolutional grid, dermed bliver et grid af flere activation til én activation pr. billede.</p>
</blockquote>

<ol>
  <li><strong>What is the “stem” of a CNN?</strong></li>
</ol>

<blockquote>
  <p>Stammen er starten af et CNN som oftest har en anden struktur end resten.</p>

  <p>Grunden til det er at man fandt ud af at størstedelen af beregning sker i de første lag. Derfor prøver man at holde stammen så simpelt og hurtigt som muligt</p>
</blockquote>

<ol>
  <li><strong>How does a bottleneck block differ from a plain ResNet block?</strong></li>
</ol>

<blockquote>
  <p>Bottleneck layers bruger 3 forskellige convolutions 2 1x1 i begyndelsen og slutningen, og en 3x3, istedet for 2 3x3.</p>
</blockquote>

<ol>
  <li><strong>Why is a bottleneck block faster?</strong></li>
</ol>

<blockquote>
  <p>Bottleneck blocks reducerer antallet af parametre og matrix multiplications.</p>
</blockquote>

<p>Hvis de 50 minutter ikke er gået så besvar følgende:</p>

<ol>
  <li>
    <p>What is “adaptive pooling”?</p>
  </li>
  <li>
    <p>What is “average pooling”?</p>
  </li>
  <li>
    <p>Why do we need Flatten after an adaptive average pooling layer?</p>
  </li>
  <li>
    <p>Why do we use plain convolutions in the CNN stem, instead of ResNet blocks?</p>
  </li>
  <li>
    <p>How do fully convolutional nets (and nets with adaptive pooling in general) allow for progressive resizing?</p>
  </li>
</ol>

<p>Uge 17</p>

<p>Opgave 1</p>

<p>I skal kode en callbackfunktion, og anvende den. Jeg har fundet følgende ønske på nettet, som jeg synes kunne være et godt eksempel:</p>

<blockquote>
  <p><em>use a condition to change a parameter (for instance change the learning rate if the validation loss hasn’t improved)</em></p>
</blockquote>

<p>Uge 18</p>

<p>Opgave 1: Done</p>

<ol>
  <li><strong>Write the Python code to implement ReLU.</strong></li>
</ol>

<p>if a &gt; 0: return a else: return 0</p>

<ol>
  <li><strong>What is the “hidden size” of a layer?</strong></li>
</ol>

<p>Repræsenterer antallet af neuroner i hvert lag i de skjulte lag.</p>

<ol>
  <li><strong>What does the t method do in PyTorch?</strong></li>
</ol>

<p>Transpose</p>

<ol>
  <li><strong>Why is matrix multiplication written in plain Python very slow?</strong></li>
</ol>

<p>For loops for daaays</p>

<p>Pytorch bruger C++ frem for Python, da Python er et langsomt sprog.</p>

<ol>
  <li><strong>In matmul, why is ac==br?</strong></li>
</ol>

<p>Der er to matricer. Førset matrices kolonner (output?) skal være samme størrelse som anden matrices rækker (Input?)</p>

<p>og hvorfor? måske fordi det ellers ikke virker?</p>

<ol>
  <li><strong>In Jupyter Notebook, how do you measure the time taken for a single cell to execute?</strong></li>
</ol>

<p>%time</p>

<ol>
  <li><strong>What is “elementwise arithmetic”?</strong></li>
</ol>

<p>basis operationer bliver lavet for hvert enkelt element hvis begge tensor har samme shape.</p>

<p>Virker kun med samem shape, eller når de er broadcastable</p>

<ol>
  <li><strong>Write the PyTorch code to test whether every element of a is greater than the corresponding element of b.</strong></li>
</ol>

<p>(a &gt; b).all()</p>

<ol>
  <li><strong>What is a rank-0 tensor? How do you convert it to a plain Python data type?</strong></li>
</ol>

<p>sum() og mean() er en reducerings operationer, som retunerer tensors med kun et element, som kunne være en boolean eller et nummer. Hvis man vil konventere det til plain python, skal man kalde på funktionen .item</p>

<p>(a + b).mean().item()</p>

<ol>
  <li><strong>What does this return, and why? tensor([1,2]) + tensor([1])</strong></li>
</ol>

<p>Det retunerer: tensor([2,3]) da de er broadcastable og dermed bliver denne tensor på højre side lavet større.</p>

<ol>
  <li><strong>What does this return, and why? tensor([1,2]) + tensor([1,2,3])</strong></li>
</ol>

<p>RuntimeError, da de ikke har samme størrelse og ikke er broadcastable</p>

<ol>
  <li><strong>How does elementwise arithmetic help us speed up matmul?</strong></li>
</ol>

<p>Vi kan slette et loop ved at gange noget med noget andet</p>

<p>I Stedet for at lave den sidste operation manuelt med loop nummer 3, hvor vi tager hvert element af de to tensor og ganger det ene med det andet, gør vi brug af elementwise arithmetic, som laver præcis det internt. Derfor kan der slettes et loop</p>

<p>Black magic og matematik!</p>

<ol>
  <li><strong>What are the broadcasting rules?</strong></li>
</ol>

<p>PyTorch sammenligner formen af elementer. Starter med at bruge <em>trailing dimensions</em> og arbejder baglæns. 2 dimensioner er kompatible når:</p>

<ol>
  <li>
    <p>De er lige (ens)</p>
  </li>
  <li>
    <p>En af dem er 1, hvor i det tilfælde den dimension er broadcast til at være det samme som den anden.</p>
  </li>
</ol>

<!-- end list -->

<ol>
  <li><strong>What is expand_as? Show an example of how it can be used to match the results of broadcasting.</strong></li>
</ol>

<p><img src="https://rbje30990.github.io/Deep_Learning/assets/img/2020-06-20-Besvarelser/media/image10.png" alt="" /></p>

<ol>
  <li><strong>How does unsqueeze help us to solve certain broadcasting problems?</strong></li>
</ol>

<p>Det giver muligheden for at broadcaste i en anden dimension, altså kolonner i stedet for rækker.</p>

<ol>
  <li><strong>How do we show the actual contents of the memory used for a tensor?</strong></li>
</ol>

<p>tensor.storage()</p>

<ol>
  <li><strong>When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)</strong></li>
</ol>

<p>Ret sikker på at det er på rows</p>

<ol>
  <li><strong>Do broadcasting and expand_as result in increased memory use? Why or why not?</strong></li>
</ol>

<p>Nej fordi at selve broadcastet ikke gemmes i en tensor i hukommelsen.</p>

<ol>
  <li><strong>What are the forward pass and backward pass of a neural network?</strong></li>
</ol>

<p>Forward pass: Udregningen. Værdi af output fra input, udregner fra første neuron til sidste. Det er modellens gæt</p>

<p>Backward pass: Udregner forandring i vægtene using gradient descent. Backpropagation. Det er modellens forsøg i at blive klogere</p>

<ol>
  <li><strong>Why do we need to store some of the activations calculated for intermediate layers in the forward pass?</strong></li>
</ol>

<p>Således at backward pass kan finde activation function</p>

<ol>
  <li><strong>What is the downside of having activations with a standard deviation too far away from 1?</strong></li>
</ol>

<p>Activations ender enten med at blive nan eller 0</p>

<ol>
  <li><strong>How can weight initialization help avoid this problem?</strong></li>
</ol>

<p>Ved at holde standard deviation af activations på 1, så man får brugbare tal.</p>

<ol>
  <li><strong>What is the formula to initialize weights such that we get a standard deviation of 1 for a plain linear layer, and for a linear layer followed by ReLU?</strong></li>
</ol>

<p>Linear: 1/kvadrat rod(n_in).</p>

<p>Med ReLU: 2/nin</p>

<ol>
  <li><strong>Why do we sometimes have to use the squeeze method in loss functions?</strong></li>
</ol>

<p>For at fjerne dimensioner med en længde af 1. Altså er det en metode til at reshape en tensor.</p>

<ol>
  <li><strong>In what order do we need to call the *_grad functions in the backward pass? Why?</strong></li>
</ol>

<p>mse, relu, lin</p>

<p>because chain rule</p>

<ol>
  <li><strong>What is __call__?</strong></li>
</ol>

<p>Indbygget Python metode. Gør at man kan skrive klasser hvor instansen er en metode og kan blive kaldt som en metode</p>

<ol>
  <li><strong>If you are mathematically inclined, find out what the gradients of a linear layer are in mathematical notation. Map that to the implementation we saw in this chapter.</strong></li>
</ol>

<p>Det er vi ikke.</p>

<ol>
  <li><strong>Implement everything in this chapter using NumPy instead of PyTorch…</strong></li>
</ol>

<p>Boi, We have no time!</p>

  </div><a class="u-url" href="/Deep_Learning/2020/06/20/Besvarelser.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/Deep_Learning/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/Deep_Learning/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/Deep_Learning/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
