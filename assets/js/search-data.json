{
  
    
        "post0": {
            "title": "Gruppespørgsmål Uge 18",
            "content": "Write the Python code to implement ReLU. | if a &gt; 0: return a else: return 0 . What is the “hidden size” of a layer? | Repræsenterer antallet af neuroner i hvert lag i de skjulte lag. . What does the t method do in PyTorch? | Transpose . Why is matrix multiplication written in plain Python very slow? | For loops for daaays . Pytorch bruger C++ frem for Python, da Python er et langsomt sprog. . In matmul, why is ac==br? | Der er to matricer. Førset matrices kolonner (output?) skal være samme størrelse som anden matrices rækker (Input?) . og hvorfor? måske fordi det ellers ikke virker? . In Jupyter Notebook, how do you measure the time taken for a single cell to execute? | %time . What is “elementwise arithmetic”? | basis operationer bliver lavet for hvert enkelt element hvis begge tensor har samme shape. . Virker kun med samem shape, eller når de er broadcastable . Write the PyTorch code to test whether every element of a is greater than the corresponding element of b. | (a &gt; b).all() . What is a rank-0 tensor? How do you convert it to a plain Python data type? | sum() og mean() er en reducerings operationer, som retunerer tensors med kun et element, som kunne være en boolean eller et nummer. Hvis man vil konventere det til plain python, skal man kalde på funktionen .item . (a + b).mean().item() . What does this return, and why? tensor([1,2]) + tensor([1]) | Det retunerer: tensor([2,3]) da de er broadcastable og dermed bliver denne tensor på højre side lavet større. . What does this return, and why? tensor([1,2]) + tensor([1,2,3]) | RuntimeError, da de ikke har samme størrelse og ikke er broadcastable . How does elementwise arithmetic help us speed up matmul? | Vi kan slette et loop ved at gange noget med noget andet . I Stedet for at lave den sidste operation manuelt med loop nummer 3, hvor vi tager hvert element af de to tensor og ganger det ene med det andet, gør vi brug af elementwise arithmetic, som laver præcis det internt. Derfor kan der slettes et loop . Black magic og matematik! . What are the broadcasting rules? | PyTorch sammenligner formen af elementer. Starter med at bruge trailing dimensions og arbejder baglæns. 2 dimensioner er kompatible når: . De er lige (ens) . | En af dem er 1, hvor i det tilfælde den dimension er broadcast til at være det samme som den anden. . | What is expand_as? Show an example of how it can be used to match the results of broadcasting. | . How does unsqueeze help us to solve certain broadcasting problems? | Det giver muligheden for at broadcaste i en anden dimension, altså kolonner i stedet for rækker. . How do we show the actual contents of the memory used for a tensor? | tensor.storage() . When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.) | Ret sikker på at det er på rows . Do broadcasting and expand_as result in increased memory use? Why or why not? | Nej fordi at selve broadcastet ikke gemmes i en tensor i hukommelsen. . What are the forward pass and backward pass of a neural network? | Forward pass: Udregningen. Værdi af output fra input, udregner fra første neuron til sidste. Det er modellens gæt . Backward pass: Udregner forandring i vægtene using gradient descent. Backpropagation. Det er modellens forsøg i at blive klogere . Why do we need to store some of the activations calculated for intermediate layers in the forward pass? | Således at backward pass kan finde activation function . What is the downside of having activations with a standard deviation too far away from 1? | Activations ender enten med at blive nan eller 0 . How can weight initialization help avoid this problem? | Ved at holde standard deviation af activations på 1, så man får brugbare tal. . What is the formula to initialize weights such that we get a standard deviation of 1 for a plain linear layer, and for a linear layer followed by ReLU? | Linear: 1/kvadrat rod(n_in). . Med ReLU: 2/nin . Why do we sometimes have to use the squeeze method in loss functions? | For at fjerne dimensioner med en længde af 1. Altså er det en metode til at reshape en tensor. . In what order do we need to call the *_grad functions in the backward pass? Why? | mse, relu, lin . because chain rule . What is __call__? | Indbygget Python metode. Gør at man kan skrive klasser hvor instansen er en metode og kan blive kaldt som en metode . If you are mathematically inclined, find out what the gradients of a linear layer are in mathematical notation. Map that to the implementation we saw in this chapter. | Det er vi ikke. . Implement everything in this chapter using NumPy instead of PyTorch… | Boi, We have no time! .",
            "url": "https://rbje30990.github.io/Deep_Learning/2020/05/04/GruppeSp%C3%B8rgsm%C3%A5l-Uge-18.html",
            "relUrl": "/2020/05/04/GruppeSp%C3%B8rgsm%C3%A5l-Uge-18.html",
            "date": " • May 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Gruppespørgsmål Uge 16",
            "content": "What is a “skip connection”? | værdierne af et output bliver til inputtet på flere lag, hvor der ikke bliver gjort noget ved disse, lag bliver altså “skippet”, når en gradient kommer for tæt på 0. . Why do skip connections allow us to train deeper models? | Det gør det muligt at undgå gradient degradation, således at der ikke kan være for mange lag i en model. . What is “identity mapping”? | Returnerer inputtet uden at ændre det . What do ResNets have to do with residuals? | ResNets prøver ikke at komme med et output fra et layer, men i stedet for med at minimere differencen mellem outputtet og det ønskede resultat. Dermed er ResNets god til at lære om der er små ændringer eller om der skal skippes. . “If the outcome of a given layer is x and we’re using a ResNet block that returns y = x + block(x), we’re not asking the block to predict y; we are asking it to predict the difference between y and x. So the job of those blocks isn’t to predict certain fea‐ tures, but to minimize the error between x and the desired y. A ResNet is, therefore, good at learning about slight differences between doing nothing and passing through a block of two convolutional layers (with trainable weights). This is how these models got their name: they’re predicting residuals (reminder: “residual” is prediction minus target).” - fra side 448 . How do we deal with the skip connection when there is a stride-2 convolution? How about when the number of filters changes? | Ved at bruge fully convolutional networks hvor man tager gennemsnittet af activations henover et convolutional grid, dermed bliver et grid af flere activation til én activation pr. billede. . What is the “stem” of a CNN? | Stammen er starten af et CNN som oftest har en anden struktur end resten. . Grunden til det er at man fandt ud af at størstedelen af beregning sker i de første lag. Derfor prøver man at holde stammen så simpelt og hurtigt som muligt . How does a bottleneck block differ from a plain ResNet block? | Bottleneck layers bruger 3 forskellige convolutions 2 1x1 i begyndelsen og slutningen, og en 3x3, istedet for 2 3x3. . Why is a bottleneck block faster? | Bottleneck blocks reducerer antallet af parametre og matrix multiplications. . Hvis de 50 minutter ikke er gået så besvar følgende: . What is “adaptive pooling”? . | What is “average pooling”? . | Why do we need Flatten after an adaptive average pooling layer? . | Why do we use plain convolutions in the CNN stem, instead of ResNet blocks? . | How do fully convolutional nets (and nets with adaptive pooling in general) allow for progressive resizing? . |",
            "url": "https://rbje30990.github.io/Deep_Learning/2020/04/20/GruppeSp%C3%B8rgsm%C3%A5l-Uge-16.html",
            "relUrl": "/2020/04/20/GruppeSp%C3%B8rgsm%C3%A5l-Uge-16.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Welcome to Jekyll!",
            "content": "test test .",
            "url": "https://rbje30990.github.io/Deep_Learning/2020/02/02/besvarelser.html",
            "relUrl": "/2020/02/02/besvarelser.html",
            "date": " • Feb 2, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://rbje30990.github.io/Deep_Learning/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rbje30990.github.io/Deep_Learning/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}